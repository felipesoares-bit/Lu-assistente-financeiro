# Lu â€¢ assistente financeiro (Next.js + Assistants API v2 + Streaming)

Front minimalista com chat central, paleta clara (azul/vermelho suave), integrando a **OpenAI Assistants API** com **streaming**. Pronto para deploy na **Vercel**.

## âœ¨ Recursos
- UI minimalista clara (fundo cinza #F3F4F6, azul #3B82F6, vermelho #EF4444)
- Chat central com avatar da assistente
- Streaming de respostas via SSE
- Assistants API v2 com `assistant_id` (threads/runs)
- Server-side seguro (Edge Runtime)

## ğŸš€ Rodando localmente
```bash
npm install
cp .env.example .env.local # edite com sua OPENAI_API_KEY e ASSISTANT_ID
npm run dev
# http://localhost:3000
```

Coloque as imagens em `public/`:
```
public/logo.png
public/assistente-virtual.png
```

## â˜ï¸ Deploy na Vercel
1. FaÃ§a fork ou suba este repo no seu GitHub.
2. Na Vercel â†’ Import Project â†’ selecione o repositÃ³rio.
3. Configure as VariÃ¡veis de Ambiente:
   - `OPENAI_API_KEY`
   - `ASSISTANT_ID`
4. Deploy.

## ğŸ”§ Notas tÃ©cnicas
- Endpoint de streaming: `POST /api/chat/stream` (SSE). O cliente lÃª `data:` com `{ delta, threadId }`.
- A API cria um **Thread** na primeira mensagem e reaproveita o `threadId` nas prÃ³ximas.
- Runtime Edge para baixa latÃªncia.

## ğŸ§© PersonalizaÃ§Ã£o
- Cores: `tailwind.config.ts`
- TÃ­tulo: `app/layout.tsx`
- Mensagem de boas-vindas: `components/Chat.tsx`

## âš ï¸ Dicas
- NÃ£o exponha sua API Key no cliente.
- Se o streaming nÃ£o chegar, verifique se a Vercel nÃ£o estÃ¡ usando CDN para a rota de API (Edge funciona com SSE por padrÃ£o) e os logs do projeto.
